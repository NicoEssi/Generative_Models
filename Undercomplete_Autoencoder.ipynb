{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Undercomplete_Autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU9IPYWIxvDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9997a86-9a3d-44e6-b328-0783c74a28f5"
      },
      "source": [
        "!git clone https://github.com/NicoEssi/Undercomplete_Autoencoder.git"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Undercomplete_Autoencoder' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuT8gTMx4kI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMio_uIrybhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set0 = pd.read_csv(\"/content/Undercomplete_Autoencoder/MNIST/bindigit_trn.csv\", header = None)\n",
        "training_set0 = np.array(training_set0, dtype = \"int\")\n",
        "\n",
        "training_set1 = pd.read_csv(\"/content/Undercomplete_Autoencoder/MNIST/targetdigit_trn.csv\", header = None)\n",
        "#training_set1 = np.array(training_set1, dtype = \"int\")\n",
        "\n",
        "test_set0 = pd.read_csv(\"/content/Undercomplete_Autoencoder/MNIST/bindigit_tst.csv\", header = None)\n",
        "test_set0 = np.array(test_set0, dtype = \"int\")\n",
        "\n",
        "test_set1 = pd.read_csv(\"/content/Undercomplete_Autoencoder/MNIST/targetdigit_tst.csv\", header = None)\n",
        "#test_set1 = np.array(test_set1, dtype = \"int\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFsKBMxAymu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "4af02400-4110-4def-9fa5-4f8149c2df98"
      },
      "source": [
        "def displayMNIST(data, index):\n",
        "    display_number = data[index, :]\n",
        "    display_number = display_number.reshape([28, 28])\n",
        "    plt.imshow(display_number, cmap='gray')\n",
        "    plt.show\n",
        "    \n",
        "def displayMNIST_alt(image):\n",
        "    image = image.reshape([28,28])\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show\n",
        "\n",
        "displayMNIST(training_set0, 135) # test 0\n",
        "#displayMNIST(test_set0, 137) # test 0"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWYrrhQqy3j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_data = torch.cuda.FloatTensor(training_set0)\n",
        "tst_data = torch.cuda.FloatTensor(test_set0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W27VKkAjzLD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoenc(nn.Module):\n",
        "    def __init__(self, nodes = 100):\n",
        "        super(autoenc, self).__init__() # inheritence\n",
        "        self.full_connection0 = nn.Linear(784, nodes) # encoding weights\n",
        "        self.full_connection1 = nn.Linear(nodes, 784) # decoding weights\n",
        "        self.activation = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.full_connection0(x)) # input encoding\n",
        "        x = self.full_connection1(x) # output decoding\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lsDcn4kzPh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = autoenc().cuda() #.cuda() - to move to GPU\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                          lr = 1e-3, weight_decay = 1/2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp2JdJ3kzYm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 500\n",
        "batch_size = 32\n",
        "length = int(len(trn_data) / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdsfJDy5MyEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1581
        },
        "outputId": "6f472b0a-541c-42b5-a358-5dde257ecea8"
      },
      "source": [
        "loss_epoch1 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    score = 0. \n",
        "    \n",
        "    \n",
        "    for num_data in range(length - 2):\n",
        "        #print(str(num_data) + \"; length = \" + str(length) + \"   num_data = \" + str(num_data))\n",
        "        batch_ind = (batch_size * num_data)\n",
        "        input = Variable(trn_data[batch_ind : batch_ind + batch_size]).cuda() #.cuda() - to move to GPU\n",
        "        # === forward propagation ===\n",
        "        #print(\"batch_ind = \" + str(batch_ind) + \"   batch_ind + batch_size = \" + str(batch_ind + batch_size))\n",
        "        output = model(input)\n",
        "        loss = criterion(output, trn_data[batch_ind : batch_ind + batch_size]) # loss between ŷ and y\n",
        "        # === backward propagation ===\n",
        "        loss.backward()\n",
        "        # === calculating epoch loss ===\n",
        "        train_loss += np.sqrt(loss.item())\n",
        "        score += 1. #<- add for average loss error instead of total\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_calculated = train_loss/score\n",
        "    print('epoch: ' + str(epoch + 1) + '   loss: ' + str(loss_calculated))\n",
        "    loss_epoch1.append(loss_calculated)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1   loss: 0.3997775120937668\n",
            "epoch: 2   loss: 0.4964393963305602\n",
            "epoch: 3   loss: 0.6337621123329102\n",
            "epoch: 4   loss: 0.8292580894618138\n",
            "epoch: 5   loss: 0.9707192269619498\n",
            "epoch: 6   loss: 1.3393757167087676\n",
            "epoch: 7   loss: 1.3370082940560901\n",
            "epoch: 8   loss: 2.0831362149471238\n",
            "epoch: 9   loss: 1.769950348289987\n",
            "epoch: 10   loss: 2.8359161799337262\n",
            "epoch: 11   loss: 3.435386702275707\n",
            "epoch: 12   loss: 2.256895454012492\n",
            "epoch: 13   loss: 5.132990871975541\n",
            "epoch: 14   loss: 4.7208915707577335\n",
            "epoch: 15   loss: 2.102017249540564\n",
            "epoch: 16   loss: 5.608499125824111\n",
            "epoch: 17   loss: 5.331111073587598\n",
            "epoch: 18   loss: 2.00847853671864\n",
            "epoch: 19   loss: 4.358589243014843\n",
            "epoch: 20   loss: 6.525645132673812\n",
            "epoch: 21   loss: 3.4937905047498106\n",
            "epoch: 22   loss: 2.0570900461115924\n",
            "epoch: 23   loss: 4.354497666316358\n",
            "epoch: 24   loss: 6.056469430437298\n",
            "epoch: 25   loss: 3.1129439546945186\n",
            "epoch: 26   loss: 2.2159605375583853\n",
            "epoch: 27   loss: 3.103001855670343\n",
            "epoch: 28   loss: 5.113193073177667\n",
            "epoch: 29   loss: 3.7756945637747554\n",
            "epoch: 30   loss: 2.1050735196213792\n",
            "epoch: 31   loss: 2.7363817504801027\n",
            "epoch: 32   loss: 2.9898196215068777\n",
            "epoch: 33   loss: 4.389321647526722\n",
            "epoch: 34   loss: 2.6538076734197023\n",
            "epoch: 35   loss: 2.477812619998838\n",
            "epoch: 36   loss: 2.227593306163099\n",
            "epoch: 37   loss: 2.872710468814357\n",
            "epoch: 38   loss: 4.008113750620117\n",
            "epoch: 39   loss: 2.415022263855006\n",
            "epoch: 40   loss: 2.6326500842565075\n",
            "epoch: 41   loss: 2.145236865177902\n",
            "epoch: 42   loss: 2.5716521348151895\n",
            "epoch: 43   loss: 3.8844846548155845\n",
            "epoch: 44   loss: 2.433320029098703\n",
            "epoch: 45   loss: 2.6556785110984196\n",
            "epoch: 46   loss: 2.3366324256995\n",
            "epoch: 47   loss: 1.9937749697704477\n",
            "epoch: 48   loss: 3.7300088423304705\n",
            "epoch: 49   loss: 2.6968028877575807\n",
            "epoch: 50   loss: 2.3868825829288682\n",
            "epoch: 51   loss: 2.761945909555154\n",
            "epoch: 52   loss: 1.790240098666641\n",
            "epoch: 53   loss: 3.2297233362852142\n",
            "epoch: 54   loss: 3.3643059268089144\n",
            "epoch: 55   loss: 2.0783340733056033\n",
            "epoch: 56   loss: 3.0454974506458927\n",
            "epoch: 57   loss: 2.552824486050381\n",
            "epoch: 58   loss: 2.268664826084138\n",
            "epoch: 59   loss: 3.9473998832723045\n",
            "epoch: 60   loss: 2.636649598223882\n",
            "epoch: 61   loss: 2.424982048244809\n",
            "epoch: 62   loss: 3.4062571699953597\n",
            "epoch: 63   loss: 2.23378724949833\n",
            "epoch: 64   loss: 3.0418816963092743\n",
            "epoch: 65   loss: 4.056817403609593\n",
            "epoch: 66   loss: 2.3781599431293885\n",
            "epoch: 67   loss: 2.8452291154003237\n",
            "epoch: 68   loss: 3.2812531240409637\n",
            "epoch: 69   loss: 2.1229881705762823\n",
            "epoch: 70   loss: 3.9131444303725353\n",
            "epoch: 71   loss: 4.0935149157597515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-00ba0943dbe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ind\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_ind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss between ŷ and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# === backward propagation ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# === calculating epoch loss ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QGrVmn21eba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        },
        "outputId": "ed7e60a1-a1de-49a3-c109-728118fd0af6"
      },
      "source": [
        "model2 = autoenc(nodes = 200).cuda() #.cuda() - to move to GPU\n",
        "optimizer2 = optim.Adam(model2.parameters(),\n",
        "                          lr = 1e-3, weight_decay = 1/2)\n",
        "num_epochs = 500\n",
        "batch_size = 32\n",
        "length = int(len(trn_data) / batch_size)\n",
        "loss_epoch2 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    score = 0. \n",
        "    \n",
        "    \n",
        "    for num_data in range(length - 2):\n",
        "        #print(str(num_data) + \"; length = \" + str(length) + \"   num_data = \" + str(num_data))\n",
        "        batch_ind = (batch_size * num_data)\n",
        "        input = Variable(trn_data[batch_ind : batch_ind + batch_size]).cuda() #.cuda() - to move to GPU\n",
        "        # === forward propagation ===\n",
        "        #print(\"batch_ind = \" + str(batch_ind) + \"   batch_ind + batch_size = \" + str(batch_ind + batch_size))\n",
        "        output = model2(input)\n",
        "        loss = criterion(output, trn_data[batch_ind : batch_ind + batch_size]) # loss between ŷ and y\n",
        "        # === backward propagation ===\n",
        "        loss.backward()\n",
        "        # === calculating epoch loss ===\n",
        "        train_loss += np.sqrt(loss.item())\n",
        "        score += 1. #<- add for average loss error instead of total\n",
        "        optimizer2.step()\n",
        "    \n",
        "    loss_calculated = train_loss/score\n",
        "    print('epoch: ' + str(epoch + 1) + '   loss: ' + str(loss_calculated))\n",
        "    loss_epoch2.append(loss_calculated)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1   loss: 0.4254735417246018\n",
            "epoch: 2   loss: 0.7038992993535769\n",
            "epoch: 3   loss: 1.1444145365358827\n",
            "epoch: 4   loss: 1.6007558264032729\n",
            "epoch: 5   loss: 2.16468619506633\n",
            "epoch: 6   loss: 2.3983529820008593\n",
            "epoch: 7   loss: 3.4203579335990324\n",
            "epoch: 8   loss: 3.6160746325784277\n",
            "epoch: 9   loss: 5.350300731216929\n",
            "epoch: 10   loss: 3.9177106240875252\n",
            "epoch: 11   loss: 6.049067394692974\n",
            "epoch: 12   loss: 3.9486047470728485\n",
            "epoch: 13   loss: 4.351804180290959\n",
            "epoch: 14   loss: 5.547305895682922\n",
            "epoch: 15   loss: 3.577722255977038\n",
            "epoch: 16   loss: 3.9709420837692364\n",
            "epoch: 17   loss: 4.411373291009745\n",
            "epoch: 18   loss: 4.267785376264218\n",
            "epoch: 19   loss: 3.4521682865216463\n",
            "epoch: 20   loss: 3.4163537300122155\n",
            "epoch: 21   loss: 4.83137262972944\n",
            "epoch: 22   loss: 3.348498934864744\n",
            "epoch: 23   loss: 3.486625487098679\n",
            "epoch: 24   loss: 3.2365865037682657\n",
            "epoch: 25   loss: 4.832805839288437\n",
            "epoch: 26   loss: 3.759088403445057\n",
            "epoch: 27   loss: 3.050516591665724\n",
            "epoch: 28   loss: 3.203400774275518\n",
            "epoch: 29   loss: 4.249283080271884\n",
            "epoch: 30   loss: 4.407158810026872\n",
            "epoch: 31   loss: 3.1335010383376787\n",
            "epoch: 32   loss: 3.5342756927099948\n",
            "epoch: 33   loss: 3.0236975824426247\n",
            "epoch: 34   loss: 4.582498698049095\n",
            "epoch: 35   loss: 3.5721588686145775\n",
            "epoch: 36   loss: 3.3683699868810977\n",
            "epoch: 37   loss: 3.5294016872058114\n",
            "epoch: 38   loss: 3.131303930161901\n",
            "epoch: 39   loss: 4.521544393667448\n",
            "epoch: 40   loss: 3.2595998706348888\n",
            "epoch: 41   loss: 3.6299370311583257\n",
            "epoch: 42   loss: 3.5028452101753986\n",
            "epoch: 43   loss: 3.0365198218542537\n",
            "epoch: 44   loss: 4.665575451046737\n",
            "epoch: 45   loss: 3.2392654869883444\n",
            "epoch: 46   loss: 3.6344693771620333\n",
            "epoch: 47   loss: 3.655752865858503\n",
            "epoch: 48   loss: 3.0025545722004883\n",
            "epoch: 49   loss: 4.673625413565696\n",
            "epoch: 50   loss: 3.6958476155809454\n",
            "epoch: 51   loss: 3.4112420247030637\n",
            "epoch: 52   loss: 3.9641427360956722\n",
            "epoch: 53   loss: 3.0177720227915485\n",
            "epoch: 54   loss: 4.167599523654152\n",
            "epoch: 55   loss: 4.282045139586254\n",
            "epoch: 56   loss: 3.426177688265451\n",
            "epoch: 57   loss: 4.049290413169327\n",
            "epoch: 58   loss: 3.4803929045739905\n",
            "epoch: 59   loss: 3.327302620257195\n",
            "epoch: 60   loss: 4.5578016499555245\n",
            "epoch: 61   loss: 3.7884646800316166\n",
            "epoch: 62   loss: 3.8642154818199574\n",
            "epoch: 63   loss: 4.017673015374021\n",
            "epoch: 64   loss: 3.2432586084086337\n",
            "epoch: 65   loss: 3.980403588876198\n",
            "epoch: 66   loss: 4.339385386087135\n",
            "epoch: 67   loss: 3.6541032489919534\n",
            "epoch: 68   loss: 4.265147187775878\n",
            "epoch: 69   loss: 3.6141724489950064\n",
            "epoch: 70   loss: 3.40482765581339\n",
            "epoch: 71   loss: 4.2774184061387945\n",
            "epoch: 72   loss: 4.03809443111625\n",
            "epoch: 73   loss: 3.996921449131668\n",
            "epoch: 74   loss: 4.101484121429575\n",
            "epoch: 75   loss: 3.4891411979633333\n",
            "epoch: 76   loss: 3.630276920753187\n",
            "epoch: 77   loss: 4.283820604286462\n",
            "epoch: 78   loss: 4.06093246552857\n",
            "epoch: 79   loss: 4.065591816717972\n",
            "epoch: 80   loss: 4.196441091281008\n",
            "epoch: 81   loss: 3.4164681864828825\n",
            "epoch: 82   loss: 3.63947970540169\n",
            "epoch: 83   loss: 4.030766230992319\n",
            "epoch: 84   loss: 4.198802530440767\n",
            "epoch: 85   loss: 4.297979273735951\n",
            "epoch: 86   loss: 3.9701902484403124\n",
            "epoch: 87   loss: 3.695934752477339\n",
            "epoch: 88   loss: 3.4455693271644106\n",
            "epoch: 89   loss: 3.9138931677624513\n",
            "epoch: 90   loss: 4.330334922961906\n",
            "epoch: 91   loss: 4.274517251824297\n",
            "epoch: 92   loss: 4.106609935312125\n",
            "epoch: 93   loss: 3.731762948162039\n",
            "epoch: 94   loss: 3.3635349617984853\n",
            "epoch: 95   loss: 3.8599256303683847\n",
            "epoch: 96   loss: 4.168300503061753\n",
            "epoch: 97   loss: 4.258710084751664\n",
            "epoch: 98   loss: 4.237779401215897\n",
            "epoch: 99   loss: 3.6604659486895037\n",
            "epoch: 100   loss: 3.5973317073257554\n",
            "epoch: 101   loss: 3.584409692862213\n",
            "epoch: 102   loss: 4.07469933401557\n",
            "epoch: 103   loss: 4.374587379465714\n",
            "epoch: 104   loss: 4.116858056046149\n",
            "epoch: 105   loss: 3.630298601220097\n",
            "epoch: 106   loss: 3.8682061205609206\n",
            "epoch: 107   loss: 3.4358600062402234\n",
            "epoch: 108   loss: 3.9855907272908477\n",
            "epoch: 109   loss: 4.363506889770902\n",
            "epoch: 110   loss: 4.087976923249011\n",
            "epoch: 111   loss: 3.9180767994697763\n",
            "epoch: 112   loss: 3.9121237985226207\n",
            "epoch: 113   loss: 3.6054833658901995\n",
            "epoch: 114   loss: 3.6590945940291735\n",
            "epoch: 115   loss: 4.48398006799243\n",
            "epoch: 116   loss: 4.229228741950908\n",
            "epoch: 117   loss: 3.7930948537898823\n",
            "epoch: 118   loss: 3.9685132971726294\n",
            "epoch: 119   loss: 3.7813799144284155\n",
            "epoch: 120   loss: 3.4736470199045497\n",
            "epoch: 121   loss: 4.087981374037702\n",
            "epoch: 122   loss: 4.530846376818788\n",
            "epoch: 123   loss: 3.951878236473056\n",
            "epoch: 124   loss: 3.840986601007506\n",
            "epoch: 125   loss: 4.049843305327169\n",
            "epoch: 126   loss: 3.1549030052568856\n",
            "epoch: 127   loss: 3.7937839946215526\n",
            "epoch: 128   loss: 4.591623179077024\n",
            "epoch: 129   loss: 4.18283711761691\n",
            "epoch: 130   loss: 3.7881857874303724\n",
            "epoch: 131   loss: 4.0368648463135965\n",
            "epoch: 132   loss: 3.4964967886496927\n",
            "epoch: 133   loss: 3.397995783160409\n",
            "epoch: 134   loss: 4.365671005440877\n",
            "epoch: 135   loss: 4.502398288017262\n",
            "epoch: 136   loss: 3.810220095572085\n",
            "epoch: 137   loss: 3.9443328192959703\n",
            "epoch: 138   loss: 3.936202676276389\n",
            "epoch: 139   loss: 3.1984822289702013\n",
            "epoch: 140   loss: 4.013403465542514\n",
            "epoch: 141   loss: 4.564939153576974\n",
            "epoch: 142   loss: 3.940993627405143\n",
            "epoch: 143   loss: 3.9192972293168715\n",
            "epoch: 144   loss: 4.229027798073961\n",
            "epoch: 145   loss: 3.5751880491210417\n",
            "epoch: 146   loss: 3.5533328721797077\n",
            "epoch: 147   loss: 4.187854460454787\n",
            "epoch: 148   loss: 4.447992189082928\n",
            "epoch: 149   loss: 3.986490687863007\n",
            "epoch: 150   loss: 4.065378365183205\n",
            "epoch: 151   loss: 4.07970594304406\n",
            "epoch: 152   loss: 3.4013358897575\n",
            "epoch: 153   loss: 3.825110571974306\n",
            "epoch: 154   loss: 4.421968037762903\n",
            "epoch: 155   loss: 4.120537841054853\n",
            "epoch: 156   loss: 3.9834023736935533\n",
            "epoch: 157   loss: 4.219543204360151\n",
            "epoch: 158   loss: 3.640729348372476\n",
            "epoch: 159   loss: 3.3595125229994354\n",
            "epoch: 160   loss: 4.305319674346948\n",
            "epoch: 161   loss: 4.313330259134278\n",
            "epoch: 162   loss: 3.7980985474073674\n",
            "epoch: 163   loss: 4.292382730585812\n",
            "epoch: 164   loss: 4.006384299863194\n",
            "epoch: 165   loss: 3.388599299340926\n",
            "epoch: 166   loss: 3.886399687056183\n",
            "epoch: 167   loss: 4.3717442210556365\n",
            "epoch: 168   loss: 3.922883514671284\n",
            "epoch: 169   loss: 4.108792842369815\n",
            "epoch: 170   loss: 4.319563142751955\n",
            "epoch: 171   loss: 3.521144981303526\n",
            "epoch: 172   loss: 3.4942422210844013\n",
            "epoch: 173   loss: 4.120716870046811\n",
            "epoch: 174   loss: 4.250917181791691\n",
            "epoch: 175   loss: 3.8266019301967926\n",
            "epoch: 176   loss: 4.409487259935388\n",
            "epoch: 177   loss: 3.9154934962687262\n",
            "epoch: 178   loss: 3.3383467450127693\n",
            "epoch: 179   loss: 3.826972860072407\n",
            "epoch: 180   loss: 4.261460086513122\n",
            "epoch: 181   loss: 3.9241003737911884\n",
            "epoch: 182   loss: 4.052075327068535\n",
            "epoch: 183   loss: 4.399796084845889\n",
            "epoch: 184   loss: 3.6561624180448877\n",
            "epoch: 185   loss: 3.455486436820778\n",
            "epoch: 186   loss: 4.082718135097713\n",
            "epoch: 187   loss: 4.15267270782501\n",
            "epoch: 188   loss: 3.7537301197282726\n",
            "epoch: 189   loss: 4.2429813985291736\n",
            "epoch: 190   loss: 4.4183202264258785\n",
            "epoch: 191   loss: 3.318978479379627\n",
            "epoch: 192   loss: 3.833878297548649\n",
            "epoch: 193   loss: 4.121075197665459\n",
            "epoch: 194   loss: 3.802781643362752\n",
            "epoch: 195   loss: 4.041685471702274\n",
            "epoch: 196   loss: 4.620961040709594\n",
            "epoch: 197   loss: 3.76330016215078\n",
            "epoch: 198   loss: 3.3977173352568757\n",
            "epoch: 199   loss: 3.9612603029547384\n",
            "epoch: 200   loss: 4.115031227764604\n",
            "epoch: 201   loss: 3.8396159590384515\n",
            "epoch: 202   loss: 4.188524442299773\n",
            "epoch: 203   loss: 4.485687358834846\n",
            "epoch: 204   loss: 3.300511857784845\n",
            "epoch: 205   loss: 3.736789580881193\n",
            "epoch: 206   loss: 4.229578786962195\n",
            "epoch: 207   loss: 3.7208928745779692\n",
            "epoch: 208   loss: 4.050584290099445\n",
            "epoch: 209   loss: 4.189595818848043\n",
            "epoch: 210   loss: 4.220429683352869\n",
            "epoch: 211   loss: 3.408161350333497\n",
            "epoch: 212   loss: 3.6908596867675643\n",
            "epoch: 213   loss: 4.307368221271315\n",
            "epoch: 214   loss: 3.642449779334504\n",
            "epoch: 215   loss: 4.053516335012467\n",
            "epoch: 216   loss: 4.394446005626284\n",
            "epoch: 217   loss: 3.9188146957576335\n",
            "epoch: 218   loss: 3.4704249267221314\n",
            "epoch: 219   loss: 3.879641169519172\n",
            "epoch: 220   loss: 4.19967971018209\n",
            "epoch: 221   loss: 3.7496312983122224\n",
            "epoch: 222   loss: 4.149475413614327\n",
            "epoch: 223   loss: 4.53658291205413\n",
            "epoch: 224   loss: 3.678232134092299\n",
            "epoch: 225   loss: 3.5550045880881163\n",
            "epoch: 226   loss: 4.401783880856466\n",
            "epoch: 227   loss: 3.6789106557109728\n",
            "epoch: 228   loss: 4.071184086586584\n",
            "epoch: 229   loss: 4.151094370741751\n",
            "epoch: 230   loss: 4.498070353010378\n",
            "epoch: 231   loss: 3.6204116370957022\n",
            "epoch: 232   loss: 3.7428604370305583\n",
            "epoch: 233   loss: 4.43176862185072\n",
            "epoch: 234   loss: 3.6901503384309624\n",
            "epoch: 235   loss: 4.196365104470648\n",
            "epoch: 236   loss: 4.396195358637922\n",
            "epoch: 237   loss: 4.19670652229079\n",
            "epoch: 238   loss: 3.6423239232486897\n",
            "epoch: 239   loss: 4.022485312183858\n",
            "epoch: 240   loss: 4.184571114915939\n",
            "epoch: 241   loss: 3.730074061048646\n",
            "epoch: 242   loss: 4.024270794586849\n",
            "epoch: 243   loss: 4.795468755967949\n",
            "epoch: 244   loss: 3.9100464186886765\n",
            "epoch: 245   loss: 3.8213316184910444\n",
            "epoch: 246   loss: 4.123456315073149\n",
            "epoch: 247   loss: 3.890279451719115\n",
            "epoch: 248   loss: 3.8627519165165802\n",
            "epoch: 249   loss: 4.093423427677508\n",
            "epoch: 250   loss: 4.718062701181463\n",
            "epoch: 251   loss: 3.8001993571312047\n",
            "epoch: 252   loss: 3.650766704415153\n",
            "epoch: 253   loss: 4.501159188426609\n",
            "epoch: 254   loss: 3.435381224647244\n",
            "epoch: 255   loss: 4.094018582914963\n",
            "epoch: 256   loss: 4.376467375843517\n",
            "epoch: 257   loss: 4.490617218985252\n",
            "epoch: 258   loss: 3.6847842789277956\n",
            "epoch: 259   loss: 3.845668511894503\n",
            "epoch: 260   loss: 4.478598924669639\n",
            "epoch: 261   loss: 3.2947340700263843\n",
            "epoch: 262   loss: 4.156932867239438\n",
            "epoch: 263   loss: 4.7311056615839755\n",
            "epoch: 264   loss: 4.127732967772457\n",
            "epoch: 265   loss: 3.69725723333249\n",
            "epoch: 266   loss: 4.202702053295978\n",
            "epoch: 267   loss: 3.9696469665896625\n",
            "epoch: 268   loss: 3.5583884197100852\n",
            "epoch: 269   loss: 4.328190794530785\n",
            "epoch: 270   loss: 4.778908159498113\n",
            "epoch: 271   loss: 3.9193101226456126\n",
            "epoch: 272   loss: 3.668874947245765\n",
            "epoch: 273   loss: 4.492880486826691\n",
            "epoch: 274   loss: 3.563812319598961\n",
            "epoch: 275   loss: 3.713430078177473\n",
            "epoch: 276   loss: 4.584251771653554\n",
            "epoch: 277   loss: 4.383921946533177\n",
            "epoch: 278   loss: 3.915924991469895\n",
            "epoch: 279   loss: 3.8599792797177\n",
            "epoch: 280   loss: 4.259549442958521\n",
            "epoch: 281   loss: 3.2942037363548087\n",
            "epoch: 282   loss: 4.089674400120176\n",
            "epoch: 283   loss: 4.6027609119994555\n",
            "epoch: 284   loss: 4.0068050719323915\n",
            "epoch: 285   loss: 3.88947133636137\n",
            "epoch: 286   loss: 4.308908656878873\n",
            "epoch: 287   loss: 3.7522207486441363\n",
            "epoch: 288   loss: 3.3633868388872408\n",
            "epoch: 289   loss: 4.366121474585537\n",
            "epoch: 290   loss: 4.355655642729003\n",
            "epoch: 291   loss: 4.021870690695322\n",
            "epoch: 292   loss: 3.891509240492452\n",
            "epoch: 293   loss: 4.42804219667319\n",
            "epoch: 294   loss: 3.45587868257981\n",
            "epoch: 295   loss: 3.5052594069251586\n",
            "epoch: 296   loss: 4.58831088144911\n",
            "epoch: 297   loss: 4.047994135590819\n",
            "epoch: 298   loss: 4.1535564600236885\n",
            "epoch: 299   loss: 4.172177385912182\n",
            "epoch: 300   loss: 4.0919770046651065\n",
            "epoch: 301   loss: 3.3708470545440044\n",
            "epoch: 302   loss: 3.7827893003399593\n",
            "epoch: 303   loss: 4.596162009648006\n",
            "epoch: 304   loss: 4.023035790274919\n",
            "epoch: 305   loss: 4.03670226008157\n",
            "epoch: 306   loss: 4.479117591236863\n",
            "epoch: 307   loss: 3.7110635664441944\n",
            "epoch: 308   loss: 3.267567411095585\n",
            "epoch: 309   loss: 4.435047051410726\n",
            "epoch: 310   loss: 4.295554551378221\n",
            "epoch: 311   loss: 4.0029632848093915\n",
            "epoch: 312   loss: 4.1560532570308615\n",
            "epoch: 313   loss: 4.5350335157379185\n",
            "epoch: 314   loss: 3.3255319370633236\n",
            "epoch: 315   loss: 3.4734694874222987\n",
            "epoch: 316   loss: 4.85478642701805\n",
            "epoch: 317   loss: 4.001725120252439\n",
            "epoch: 318   loss: 4.120761421572391\n",
            "epoch: 319   loss: 4.517390096213401\n",
            "epoch: 320   loss: 3.960655448022521\n",
            "epoch: 321   loss: 3.369015572473914\n",
            "epoch: 322   loss: 3.6720972232866362\n",
            "epoch: 323   loss: 5.18769106426147\n",
            "epoch: 324   loss: 3.817123667947965\n",
            "epoch: 325   loss: 4.059171039560438\n",
            "epoch: 326   loss: 4.940663713709065\n",
            "epoch: 327   loss: 3.42300158830462\n",
            "epoch: 328   loss: 3.3763627826454057\n",
            "epoch: 329   loss: 4.4971433978961555\n",
            "epoch: 330   loss: 4.553574107045725\n",
            "epoch: 331   loss: 3.8366166955998535\n",
            "epoch: 332   loss: 4.489886077286889\n",
            "epoch: 333   loss: 4.851225977521857\n",
            "epoch: 334   loss: 3.1596748254978446\n",
            "epoch: 335   loss: 3.6285722825844346\n",
            "epoch: 336   loss: 5.0868943681302845\n",
            "epoch: 337   loss: 3.9380634743047582\n",
            "epoch: 338   loss: 4.127588676650081\n",
            "epoch: 339   loss: 4.9994906833700465\n",
            "epoch: 340   loss: 4.07872630423009\n",
            "epoch: 341   loss: 3.396334646599354\n",
            "epoch: 342   loss: 3.7118656087152297\n",
            "epoch: 343   loss: 5.125831429747967\n",
            "epoch: 344   loss: 3.9144450884233803\n",
            "epoch: 345   loss: 4.265869622446183\n",
            "epoch: 346   loss: 5.289665625072554\n",
            "epoch: 347   loss: 3.4122314970757928\n",
            "epoch: 348   loss: 3.5134224059002066\n",
            "epoch: 349   loss: 4.3665022480798825\n",
            "epoch: 350   loss: 4.600919041064645\n",
            "epoch: 351   loss: 4.034589611454069\n",
            "epoch: 352   loss: 4.328832590957607\n",
            "epoch: 353   loss: 5.169791538835531\n",
            "epoch: 354   loss: 3.0063924361367302\n",
            "epoch: 355   loss: 3.7354012581114153\n",
            "epoch: 356   loss: 5.107589607279779\n",
            "epoch: 357   loss: 3.758458144554647\n",
            "epoch: 358   loss: 4.1939061025202085\n",
            "epoch: 359   loss: 4.91609693860898\n",
            "epoch: 360   loss: 4.272673220104221\n",
            "epoch: 361   loss: 3.251946098162416\n",
            "epoch: 362   loss: 4.03255150516036\n",
            "epoch: 363   loss: 4.736799815721571\n",
            "epoch: 364   loss: 3.7233409186072723\n",
            "epoch: 365   loss: 4.4570149539136965\n",
            "epoch: 366   loss: 5.42700403932259\n",
            "epoch: 367   loss: 3.431919415217508\n",
            "epoch: 368   loss: 3.5925895836191977\n",
            "epoch: 369   loss: 4.604318224820301\n",
            "epoch: 370   loss: 3.945663315961017\n",
            "epoch: 371   loss: 4.188841965420019\n",
            "epoch: 372   loss: 4.569520835191584\n",
            "epoch: 373   loss: 5.135539890285579\n",
            "epoch: 374   loss: 3.1555986923664654\n",
            "epoch: 375   loss: 3.7859434829373035\n",
            "epoch: 376   loss: 5.144725749083675\n",
            "epoch: 377   loss: 3.0932510746443618\n",
            "epoch: 378   loss: 4.451146173156475\n",
            "epoch: 379   loss: 5.254946003142074\n",
            "epoch: 380   loss: 4.304970675112402\n",
            "epoch: 381   loss: 3.460109871032931\n",
            "epoch: 382   loss: 4.227008516611309\n",
            "epoch: 383   loss: 4.389070310679212\n",
            "epoch: 384   loss: 3.411887368815735\n",
            "epoch: 385   loss: 4.479203910407074\n",
            "epoch: 386   loss: 5.580870356902052\n",
            "epoch: 387   loss: 3.6483695197715122\n",
            "epoch: 388   loss: 3.435714660756761\n",
            "epoch: 389   loss: 4.903225304797838\n",
            "epoch: 390   loss: 3.2941143128023795\n",
            "epoch: 391   loss: 3.923446789683888\n",
            "epoch: 392   loss: 5.127722393399038\n",
            "epoch: 393   loss: 5.024356606943104\n",
            "epoch: 394   loss: 3.419750951534663\n",
            "epoch: 395   loss: 3.551266828130993\n",
            "epoch: 396   loss: 5.227548605484237\n",
            "epoch: 397   loss: 2.6597581432045216\n",
            "epoch: 398   loss: 4.221523837518951\n",
            "epoch: 399   loss: 5.964179616280738\n",
            "epoch: 400   loss: 4.11342063321557\n",
            "epoch: 401   loss: 3.439906492231804\n",
            "epoch: 402   loss: 4.42771557770601\n",
            "epoch: 403   loss: 4.10000461117643\n",
            "epoch: 404   loss: 2.9309749924258073\n",
            "epoch: 405   loss: 4.772813478291272\n",
            "epoch: 406   loss: 5.736009251661499\n",
            "epoch: 407   loss: 3.693839127011743\n",
            "epoch: 408   loss: 3.39797474858828\n",
            "epoch: 409   loss: 5.440957173190763\n",
            "epoch: 410   loss: 2.984924249633897\n",
            "epoch: 411   loss: 3.438851834141546\n",
            "epoch: 412   loss: 5.408846771277111\n",
            "epoch: 413   loss: 4.871403621222186\n",
            "epoch: 414   loss: 3.71109729893804\n",
            "epoch: 415   loss: 3.564638159218261\n",
            "epoch: 416   loss: 5.403250961408267\n",
            "epoch: 417   loss: 2.604535370501711\n",
            "epoch: 418   loss: 3.7641594597922277\n",
            "epoch: 419   loss: 6.280346770276111\n",
            "epoch: 420   loss: 3.8365463292884403\n",
            "epoch: 421   loss: 3.5623165073224503\n",
            "epoch: 422   loss: 4.793050450333188\n",
            "epoch: 423   loss: 4.100968858358453\n",
            "epoch: 424   loss: 2.7435439649766726\n",
            "epoch: 425   loss: 4.677399331417644\n",
            "epoch: 426   loss: 5.901060842808852\n",
            "epoch: 427   loss: 3.5015459684054018\n",
            "epoch: 428   loss: 3.4943300632732384\n",
            "epoch: 429   loss: 5.890448855759308\n",
            "epoch: 430   loss: 2.7275833077303093\n",
            "epoch: 431   loss: 3.0378617275332465\n",
            "epoch: 432   loss: 6.270487003476214\n",
            "epoch: 433   loss: 4.493015193434546\n",
            "epoch: 434   loss: 3.4550420716379207\n",
            "epoch: 435   loss: 4.140086439036471\n",
            "epoch: 436   loss: 5.353288877381017\n",
            "epoch: 437   loss: 2.4339904553966742\n",
            "epoch: 438   loss: 3.540634248295224\n",
            "epoch: 439   loss: 6.975457341204938\n",
            "epoch: 440   loss: 3.471481921605633\n",
            "epoch: 441   loss: 3.5256982847526492\n",
            "epoch: 442   loss: 5.853803875618128\n",
            "epoch: 443   loss: 3.6632196022091716\n",
            "epoch: 444   loss: 2.4378256110726055\n",
            "epoch: 445   loss: 4.731709549762983\n",
            "epoch: 446   loss: 6.154898141423415\n",
            "epoch: 447   loss: 3.022212869504649\n",
            "epoch: 448   loss: 4.186287498670466\n",
            "epoch: 449   loss: 6.292868245187745\n",
            "epoch: 450   loss: 2.198400178171288\n",
            "epoch: 451   loss: 2.5507902191759753\n",
            "epoch: 452   loss: 6.955633836966451\n",
            "epoch: 453   loss: 4.067158268142719\n",
            "epoch: 454   loss: 3.007147805690219\n",
            "epoch: 455   loss: 5.8651130372729785\n",
            "epoch: 456   loss: 4.6689921653841076\n",
            "epoch: 457   loss: 1.8053211851344586\n",
            "epoch: 458   loss: 3.6905858936842866\n",
            "epoch: 459   loss: 7.063264525569776\n",
            "epoch: 460   loss: 3.0689309722940985\n",
            "epoch: 461   loss: 3.489014111507317\n",
            "epoch: 462   loss: 7.027800511438411\n",
            "epoch: 463   loss: 2.4599691096006513\n",
            "epoch: 464   loss: 2.1370422998053002\n",
            "epoch: 465   loss: 6.175126499664723\n",
            "epoch: 466   loss: 4.911765358280504\n",
            "epoch: 467   loss: 2.8860921573377394\n",
            "epoch: 468   loss: 5.072937594971931\n",
            "epoch: 469   loss: 5.9408217280120414\n",
            "epoch: 470   loss: 1.7253341261720723\n",
            "epoch: 471   loss: 2.8884599395441337\n",
            "epoch: 472   loss: 7.1857934631682285\n",
            "epoch: 473   loss: 3.396060657479857\n",
            "epoch: 474   loss: 3.2374730339377926\n",
            "epoch: 475   loss: 7.255907343626528\n",
            "epoch: 476   loss: 3.137585260382798\n",
            "epoch: 477   loss: 1.6206746663813703\n",
            "epoch: 478   loss: 5.523192295526785\n",
            "epoch: 479   loss: 5.207876114796082\n",
            "epoch: 480   loss: 2.946824979386829\n",
            "epoch: 481   loss: 4.6420101661758055\n",
            "epoch: 482   loss: 6.658202961751947\n",
            "epoch: 483   loss: 1.8949506725435776\n",
            "epoch: 484   loss: 2.76174616330567\n",
            "epoch: 485   loss: 6.894656952629086\n",
            "epoch: 486   loss: 3.0798569627790764\n",
            "epoch: 487   loss: 3.2800343322857635\n",
            "epoch: 488   loss: 6.901848166981823\n",
            "epoch: 489   loss: 4.0197370176195495\n",
            "epoch: 490   loss: 1.8229122989724433\n",
            "epoch: 491   loss: 5.618619630694051\n",
            "epoch: 492   loss: 4.15621161525908\n",
            "epoch: 493   loss: 2.8260508813917102\n",
            "epoch: 494   loss: 5.0898964278410945\n",
            "epoch: 495   loss: 5.826664240539643\n",
            "epoch: 496   loss: 2.5243163003640965\n",
            "epoch: 497   loss: 3.726114382508823\n",
            "epoch: 498   loss: 5.610387520258483\n",
            "epoch: 499   loss: 2.4163738589321526\n",
            "epoch: 500   loss: 4.181074595172604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo5MPXoE2PrA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8517
        },
        "outputId": "12bef368-845e-46c6-fa91-7028c4f60058"
      },
      "source": [
        "model3 = autoenc(nodes = 400).cuda() #.cuda() - to move to GPU\n",
        "optimizer3 = optim.Adam(model3.parameters(),\n",
        "                          lr = 1e-3, weight_decay = 1/2)\n",
        "num_epochs = 500\n",
        "batch_size = 32\n",
        "length = int(len(trn_data) / batch_size)\n",
        "loss_epoch3 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    score = 0. \n",
        "    \n",
        "    \n",
        "    for num_data in range(length - 2):\n",
        "        #print(str(num_data) + \"; length = \" + str(length) + \"   num_data = \" + str(num_data))\n",
        "        batch_ind = (batch_size * num_data)\n",
        "        input = Variable(trn_data[batch_ind : batch_ind + batch_size]).cuda() #.cuda() - to move to GPU\n",
        "        # === forward propagation ===\n",
        "        #print(\"batch_ind = \" + str(batch_ind) + \"   batch_ind + batch_size = \" + str(batch_ind + batch_size))\n",
        "        output = model3(input)\n",
        "        loss = criterion(output, trn_data[batch_ind : batch_ind + batch_size]) # loss between ŷ and y\n",
        "        # === backward propagation ===\n",
        "        loss.backward()\n",
        "        # === calculating epoch loss ===\n",
        "        train_loss += np.sqrt(loss.item())\n",
        "        score += 1. #<- add for average loss error instead of total\n",
        "        optimizer3.step()\n",
        "    \n",
        "    loss_calculated = train_loss/score\n",
        "    print('epoch: ' + str(epoch + 1) + '   loss: ' + str(loss_calculated))\n",
        "    loss_epoch3.append(loss_calculated)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1   loss: 0.5247461085057042\n",
            "epoch: 2   loss: 1.4605352114473766\n",
            "epoch: 3   loss: 2.4067489098767068\n",
            "epoch: 4   loss: 3.1380160773835097\n",
            "epoch: 5   loss: 3.978485064561539\n",
            "epoch: 6   loss: 4.628384894753286\n",
            "epoch: 7   loss: 4.761361696615095\n",
            "epoch: 8   loss: 4.330400488538271\n",
            "epoch: 9   loss: 4.448761431176256\n",
            "epoch: 10   loss: 4.921544470592476\n",
            "epoch: 11   loss: 5.076834990735261\n",
            "epoch: 12   loss: 4.49894023342423\n",
            "epoch: 13   loss: 4.0852427291270095\n",
            "epoch: 14   loss: 5.3882633295310844\n",
            "epoch: 15   loss: 4.443376528074276\n",
            "epoch: 16   loss: 3.899694477761142\n",
            "epoch: 17   loss: 5.384332194813509\n",
            "epoch: 18   loss: 3.801834612076792\n",
            "epoch: 19   loss: 4.977993906235681\n",
            "epoch: 20   loss: 4.399160696197027\n",
            "epoch: 21   loss: 4.623285107864945\n",
            "epoch: 22   loss: 4.511077095887783\n",
            "epoch: 23   loss: 4.897863816223229\n",
            "epoch: 24   loss: 4.161868210152896\n",
            "epoch: 25   loss: 5.241638085566545\n",
            "epoch: 26   loss: 4.117279446233237\n",
            "epoch: 27   loss: 5.358016611636538\n",
            "epoch: 28   loss: 4.285969743171484\n",
            "epoch: 29   loss: 5.308528139391909\n",
            "epoch: 30   loss: 4.387578395174141\n",
            "epoch: 31   loss: 5.111217893280111\n",
            "epoch: 32   loss: 4.823700809801889\n",
            "epoch: 33   loss: 4.783101456567074\n",
            "epoch: 34   loss: 5.2188570080546866\n",
            "epoch: 35   loss: 4.628773076417304\n",
            "epoch: 36   loss: 5.24431211190406\n",
            "epoch: 37   loss: 4.988978181644126\n",
            "epoch: 38   loss: 4.908714214594443\n",
            "epoch: 39   loss: 5.297831310152284\n",
            "epoch: 40   loss: 4.763106254296989\n",
            "epoch: 41   loss: 5.375470110141637\n",
            "epoch: 42   loss: 5.070624755211405\n",
            "epoch: 43   loss: 4.993854920028611\n",
            "epoch: 44   loss: 5.366988334352171\n",
            "epoch: 45   loss: 4.929607774021659\n",
            "epoch: 46   loss: 5.468121427968965\n",
            "epoch: 47   loss: 5.118488508535005\n",
            "epoch: 48   loss: 5.144487278785181\n",
            "epoch: 49   loss: 5.4981180863706856\n",
            "epoch: 50   loss: 5.073099378165488\n",
            "epoch: 51   loss: 5.358360611981953\n",
            "epoch: 52   loss: 5.276654649785055\n",
            "epoch: 53   loss: 5.317314102307335\n",
            "epoch: 54   loss: 5.312318447032502\n",
            "epoch: 55   loss: 5.347383186424473\n",
            "epoch: 56   loss: 5.323916011229178\n",
            "epoch: 57   loss: 5.490262049457298\n",
            "epoch: 58   loss: 5.350247042496681\n",
            "epoch: 59   loss: 5.150966810811006\n",
            "epoch: 60   loss: 5.575002765665998\n",
            "epoch: 61   loss: 5.474127318798209\n",
            "epoch: 62   loss: 5.338048525565955\n",
            "epoch: 63   loss: 5.442260627588468\n",
            "epoch: 64   loss: 5.4396471000513\n",
            "epoch: 65   loss: 5.24573260720961\n",
            "epoch: 66   loss: 5.540109840267518\n",
            "epoch: 67   loss: 5.566417866654179\n",
            "epoch: 68   loss: 5.3197523898707875\n",
            "epoch: 69   loss: 5.460240529922024\n",
            "epoch: 70   loss: 5.314600756677939\n",
            "epoch: 71   loss: 5.401869999446498\n",
            "epoch: 72   loss: 5.704381665376509\n",
            "epoch: 73   loss: 5.291302624274989\n",
            "epoch: 74   loss: 5.489336532160747\n",
            "epoch: 75   loss: 5.487764452605096\n",
            "epoch: 76   loss: 5.386542575475963\n",
            "epoch: 77   loss: 5.511486587732948\n",
            "epoch: 78   loss: 5.559114283952812\n",
            "epoch: 79   loss: 5.428201525989538\n",
            "epoch: 80   loss: 5.436158874695589\n",
            "epoch: 81   loss: 5.647351878068777\n",
            "epoch: 82   loss: 5.267592599308493\n",
            "epoch: 83   loss: 5.626053729953706\n",
            "epoch: 84   loss: 5.70662463985177\n",
            "epoch: 85   loss: 5.407740617288609\n",
            "epoch: 86   loss: 5.542682557135382\n",
            "epoch: 87   loss: 5.625200474092759\n",
            "epoch: 88   loss: 5.439051421387098\n",
            "epoch: 89   loss: 5.651976270603885\n",
            "epoch: 90   loss: 5.6894371580426775\n",
            "epoch: 91   loss: 5.484606319035261\n",
            "epoch: 92   loss: 5.488451912784256\n",
            "epoch: 93   loss: 5.8747674161521655\n",
            "epoch: 94   loss: 5.414921382898979\n",
            "epoch: 95   loss: 5.6697746111543275\n",
            "epoch: 96   loss: 5.798436802240235\n",
            "epoch: 97   loss: 5.63710443003327\n",
            "epoch: 98   loss: 5.643240246000645\n",
            "epoch: 99   loss: 5.607324556724432\n",
            "epoch: 100   loss: 5.831663647787927\n",
            "epoch: 101   loss: 5.58367870205698\n",
            "epoch: 102   loss: 5.798827887217019\n",
            "epoch: 103   loss: 5.969497525094299\n",
            "epoch: 104   loss: 5.399159187812013\n",
            "epoch: 105   loss: 5.733491065671953\n",
            "epoch: 106   loss: 6.170629903236058\n",
            "epoch: 107   loss: 5.299900750104806\n",
            "epoch: 108   loss: 5.832396675882026\n",
            "epoch: 109   loss: 6.263225858186279\n",
            "epoch: 110   loss: 5.205338106605428\n",
            "epoch: 111   loss: 6.018788447802575\n",
            "epoch: 112   loss: 6.132956206023858\n",
            "epoch: 113   loss: 5.159904680707987\n",
            "epoch: 114   loss: 6.080037303656111\n",
            "epoch: 115   loss: 6.1822473784912235\n",
            "epoch: 116   loss: 5.442117703255897\n",
            "epoch: 117   loss: 6.00731248848112\n",
            "epoch: 118   loss: 5.9488685546341005\n",
            "epoch: 119   loss: 5.755489475093042\n",
            "epoch: 120   loss: 5.740840969306313\n",
            "epoch: 121   loss: 6.054263238208339\n",
            "epoch: 122   loss: 6.084897424624293\n",
            "epoch: 123   loss: 5.3657360388459026\n",
            "epoch: 124   loss: 6.137690218597842\n",
            "epoch: 125   loss: 6.205061775465958\n",
            "epoch: 126   loss: 5.054606970575228\n",
            "epoch: 127   loss: 6.348749175521906\n",
            "epoch: 128   loss: 6.417134167521295\n",
            "epoch: 129   loss: 5.084909800277502\n",
            "epoch: 130   loss: 6.352037275447502\n",
            "epoch: 131   loss: 6.289407878742083\n",
            "epoch: 132   loss: 5.145467177339268\n",
            "epoch: 133   loss: 6.386311744487311\n",
            "epoch: 134   loss: 6.083134521571405\n",
            "epoch: 135   loss: 5.772696919230648\n",
            "epoch: 136   loss: 5.972858077777355\n",
            "epoch: 137   loss: 6.054990284392607\n",
            "epoch: 138   loss: 6.055995474734298\n",
            "epoch: 139   loss: 5.53816625756376\n",
            "epoch: 140   loss: 6.278567622598634\n",
            "epoch: 141   loss: 6.366712851771282\n",
            "epoch: 142   loss: 5.340890702675034\n",
            "epoch: 143   loss: 6.349708796519533\n",
            "epoch: 144   loss: 6.46287218701767\n",
            "epoch: 145   loss: 4.947723382098227\n",
            "epoch: 146   loss: 6.690370393962171\n",
            "epoch: 147   loss: 6.282190928102865\n",
            "epoch: 148   loss: 5.230730782949756\n",
            "epoch: 149   loss: 6.63928050749282\n",
            "epoch: 150   loss: 5.951641436331959\n",
            "epoch: 151   loss: 5.531888517191066\n",
            "epoch: 152   loss: 6.4044185600081835\n",
            "epoch: 153   loss: 5.983528771582921\n",
            "epoch: 154   loss: 6.007857481214411\n",
            "epoch: 155   loss: 5.763537396457717\n",
            "epoch: 156   loss: 6.297976963951852\n",
            "epoch: 157   loss: 6.304631939224853\n",
            "epoch: 158   loss: 5.233328668332765\n",
            "epoch: 159   loss: 6.589546769234582\n",
            "epoch: 160   loss: 6.21458380277135\n",
            "epoch: 161   loss: 5.246122313919272\n",
            "epoch: 162   loss: 6.843100810500661\n",
            "epoch: 163   loss: 6.283714595282872\n",
            "epoch: 164   loss: 5.156189974878926\n",
            "epoch: 165   loss: 6.516712533957795\n",
            "epoch: 166   loss: 6.007796359141067\n",
            "epoch: 167   loss: 5.882941033250815\n",
            "epoch: 168   loss: 6.138048795323785\n",
            "epoch: 169   loss: 6.086150284891982\n",
            "epoch: 170   loss: 6.447556510038139\n",
            "epoch: 171   loss: 5.327850230562567\n",
            "epoch: 172   loss: 6.422281565157443\n",
            "epoch: 173   loss: 6.4875333433823075\n",
            "epoch: 174   loss: 5.215738800650306\n",
            "epoch: 175   loss: 6.815968838922454\n",
            "epoch: 176   loss: 6.217851321394802\n",
            "epoch: 177   loss: 5.4184416968542\n",
            "epoch: 178   loss: 6.893976220617121\n",
            "epoch: 179   loss: 5.609869892753454\n",
            "epoch: 180   loss: 6.218136925200228\n",
            "epoch: 181   loss: 6.425971108277286\n",
            "epoch: 182   loss: 5.569239862246011\n",
            "epoch: 183   loss: 6.891833524032801\n",
            "epoch: 184   loss: 5.5718164935495285\n",
            "epoch: 185   loss: 6.08300945726589\n",
            "epoch: 186   loss: 6.905223220757403\n",
            "epoch: 187   loss: 5.272960345768792\n",
            "epoch: 188   loss: 6.789929223092362\n",
            "epoch: 189   loss: 5.9667364821946265\n",
            "epoch: 190   loss: 5.793761685655377\n",
            "epoch: 191   loss: 6.944500461323632\n",
            "epoch: 192   loss: 5.175725514192758\n",
            "epoch: 193   loss: 6.719043286189176\n",
            "epoch: 194   loss: 6.5564217501337945\n",
            "epoch: 195   loss: 5.025313543102794\n",
            "epoch: 196   loss: 7.343923300419635\n",
            "epoch: 197   loss: 5.87839462818995\n",
            "epoch: 198   loss: 5.431453736201314\n",
            "epoch: 199   loss: 7.2588341563832754\n",
            "epoch: 200   loss: 5.44592233485796\n",
            "epoch: 201   loss: 6.5910509732415274\n",
            "epoch: 202   loss: 6.106290568891938\n",
            "epoch: 203   loss: 6.099937461631576\n",
            "epoch: 204   loss: 6.9873557771262425\n",
            "epoch: 205   loss: 4.680839437139164\n",
            "epoch: 206   loss: 7.240788515110165\n",
            "epoch: 207   loss: 6.8703332837741025\n",
            "epoch: 208   loss: 4.6270501401500255\n",
            "epoch: 209   loss: 7.830961862420859\n",
            "epoch: 210   loss: 6.204958994387485\n",
            "epoch: 211   loss: 5.286543645647741\n",
            "epoch: 212   loss: 6.977589960384983\n",
            "epoch: 213   loss: 5.960566041200393\n",
            "epoch: 214   loss: 6.560960978569321\n",
            "epoch: 215   loss: 5.817514974949277\n",
            "epoch: 216   loss: 6.674222316350689\n",
            "epoch: 217   loss: 7.235750839636468\n",
            "epoch: 218   loss: 4.326481766144606\n",
            "epoch: 219   loss: 7.592087613406216\n",
            "epoch: 220   loss: 7.018491951981305\n",
            "epoch: 221   loss: 4.3217530640348025\n",
            "epoch: 222   loss: 8.244177854938295\n",
            "epoch: 223   loss: 6.31277442714511\n",
            "epoch: 224   loss: 5.012230465293584\n",
            "epoch: 225   loss: 7.043725830849427\n",
            "epoch: 226   loss: 6.127613521200993\n",
            "epoch: 227   loss: 6.5854815193574705\n",
            "epoch: 228   loss: 5.61450459680338\n",
            "epoch: 229   loss: 7.014395172219756\n",
            "epoch: 230   loss: 7.027001781188955\n",
            "epoch: 231   loss: 3.947148031344057\n",
            "epoch: 232   loss: 8.113395047530934\n",
            "epoch: 233   loss: 7.072088981251038\n",
            "epoch: 234   loss: 4.016086535266817\n",
            "epoch: 235   loss: 8.330960113632909\n",
            "epoch: 236   loss: 6.704965014854447\n",
            "epoch: 237   loss: 4.59136925198394\n",
            "epoch: 238   loss: 7.077119791598041\n",
            "epoch: 239   loss: 6.859842667449183\n",
            "epoch: 240   loss: 5.978089387986818\n",
            "epoch: 241   loss: 5.6480214875141055\n",
            "epoch: 242   loss: 7.430795933399022\n",
            "epoch: 243   loss: 6.738427778253143\n",
            "epoch: 244   loss: 4.082917640350358\n",
            "epoch: 245   loss: 8.213567413902053\n",
            "epoch: 246   loss: 7.225889778679261\n",
            "epoch: 247   loss: 3.856697917054644\n",
            "epoch: 248   loss: 8.648032922212964\n",
            "epoch: 249   loss: 7.127005125995108\n",
            "epoch: 250   loss: 4.125235847340297\n",
            "epoch: 251   loss: 7.438475082367251\n",
            "epoch: 252   loss: 7.1001819038073615\n",
            "epoch: 253   loss: 5.5162817491860405\n",
            "epoch: 254   loss: 6.215194846071578\n",
            "epoch: 255   loss: 7.677409499052614\n",
            "epoch: 256   loss: 6.456575217754174\n",
            "epoch: 257   loss: 4.133704693378599\n",
            "epoch: 258   loss: 8.628278285967976\n",
            "epoch: 259   loss: 6.5921331321332755\n",
            "epoch: 260   loss: 3.8466446103954395\n",
            "epoch: 261   loss: 8.905319000562095\n",
            "epoch: 262   loss: 6.852907127505664\n",
            "epoch: 263   loss: 3.9765800736603327\n",
            "epoch: 264   loss: 7.5806363612111545\n",
            "epoch: 265   loss: 7.25963270891299\n",
            "epoch: 266   loss: 4.157067007666921\n",
            "epoch: 267   loss: 6.147416390548536\n",
            "epoch: 268   loss: 8.444599370883735\n",
            "epoch: 269   loss: 4.716379407370921\n",
            "epoch: 270   loss: 4.413169056452039\n",
            "epoch: 271   loss: 9.508200701333779\n",
            "epoch: 272   loss: 4.785887223428121\n",
            "epoch: 273   loss: 3.6194808902325946\n",
            "epoch: 274   loss: 9.889473334262975\n",
            "epoch: 275   loss: 5.092222006886605\n",
            "epoch: 276   loss: 3.299776346282763\n",
            "epoch: 277   loss: 8.978435884238028\n",
            "epoch: 278   loss: 6.1573654649158875\n",
            "epoch: 279   loss: 3.288403300929785\n",
            "epoch: 280   loss: 7.117270393399131\n",
            "epoch: 281   loss: 7.979498370600118\n",
            "epoch: 282   loss: 2.9777619779146587\n",
            "epoch: 283   loss: 5.252439901166489\n",
            "epoch: 284   loss: 9.73348600694975\n",
            "epoch: 285   loss: 3.1938527657748095\n",
            "epoch: 286   loss: 4.189933763469955\n",
            "epoch: 287   loss: 10.44036828581705\n",
            "epoch: 288   loss: 3.5468285554080357\n",
            "epoch: 289   loss: 3.108563235779977\n",
            "epoch: 290   loss: 10.11484409067988\n",
            "epoch: 291   loss: 4.996489887343729\n",
            "epoch: 292   loss: 2.7598791764080124\n",
            "epoch: 293   loss: 9.016608805038315\n",
            "epoch: 294   loss: 6.890810888231027\n",
            "epoch: 295   loss: 2.2865161913455374\n",
            "epoch: 296   loss: 6.38974312570535\n",
            "epoch: 297   loss: 9.311226890611925\n",
            "epoch: 298   loss: 2.518075626436568\n",
            "epoch: 299   loss: 4.410054288002633\n",
            "epoch: 300   loss: 11.409975574262003\n",
            "epoch: 301   loss: 3.202213192807183\n",
            "epoch: 302   loss: 2.812371472927412\n",
            "epoch: 303   loss: 11.51576651973871\n",
            "epoch: 304   loss: 4.327339341399138\n",
            "epoch: 305   loss: 2.346936514560886\n",
            "epoch: 306   loss: 9.922256704935622\n",
            "epoch: 307   loss: 6.944663870874203\n",
            "epoch: 308   loss: 2.4381854415991757\n",
            "epoch: 309   loss: 7.4494768677112155\n",
            "epoch: 310   loss: 9.232208671181287\n",
            "epoch: 311   loss: 2.5571466768942375\n",
            "epoch: 312   loss: 5.221872325185164\n",
            "epoch: 313   loss: 10.572128922451562\n",
            "epoch: 314   loss: 3.365888581063554\n",
            "epoch: 315   loss: 4.008261471916604\n",
            "epoch: 316   loss: 11.34635930403748\n",
            "epoch: 317   loss: 4.216408490141738\n",
            "epoch: 318   loss: 3.115244425004156\n",
            "epoch: 319   loss: 10.209717837007311\n",
            "epoch: 320   loss: 5.070220375900525\n",
            "epoch: 321   loss: 2.9361943384272653\n",
            "epoch: 322   loss: 9.492360843403628\n",
            "epoch: 323   loss: 6.732430653816292\n",
            "epoch: 324   loss: 2.6163142334688705\n",
            "epoch: 325   loss: 7.829989313179408\n",
            "epoch: 326   loss: 7.196589962755149\n",
            "epoch: 327   loss: 2.972269976742467\n",
            "epoch: 328   loss: 6.7878332339202725\n",
            "epoch: 329   loss: 8.59301984915449\n",
            "epoch: 330   loss: 3.2853251337727762\n",
            "epoch: 331   loss: 5.356073202962753\n",
            "epoch: 332   loss: 8.502331947881492\n",
            "epoch: 333   loss: 3.6047521751711975\n",
            "epoch: 334   loss: 5.403131658435895\n",
            "epoch: 335   loss: 8.378369965943175\n",
            "epoch: 336   loss: 4.14052714927941\n",
            "epoch: 337   loss: 5.0668213115157155\n",
            "epoch: 338   loss: 7.934917545001656\n",
            "epoch: 339   loss: 4.354489558005244\n",
            "epoch: 340   loss: 5.067703052173069\n",
            "epoch: 341   loss: 7.603568967676856\n",
            "epoch: 342   loss: 4.167460218695113\n",
            "epoch: 343   loss: 5.338304398461854\n",
            "epoch: 344   loss: 7.436422530218812\n",
            "epoch: 345   loss: 4.311422457396412\n",
            "epoch: 346   loss: 5.369544743081763\n",
            "epoch: 347   loss: 7.0315404314444\n",
            "epoch: 348   loss: 4.342756533464923\n",
            "epoch: 349   loss: 5.356256867227951\n",
            "epoch: 350   loss: 7.351201169897364\n",
            "epoch: 351   loss: 4.531565995147937\n",
            "epoch: 352   loss: 4.974281457252762\n",
            "epoch: 353   loss: 7.120819256792531\n",
            "epoch: 354   loss: 4.6573207368948815\n",
            "epoch: 355   loss: 4.7353708949124265\n",
            "epoch: 356   loss: 7.277287038120291\n",
            "epoch: 357   loss: 5.449268829295274\n",
            "epoch: 358   loss: 4.215080305606752\n",
            "epoch: 359   loss: 6.921444523954262\n",
            "epoch: 360   loss: 5.624116447238832\n",
            "epoch: 361   loss: 4.017812777936623\n",
            "epoch: 362   loss: 6.356648664283377\n",
            "epoch: 363   loss: 6.9114131001489145\n",
            "epoch: 364   loss: 3.929729250077687\n",
            "epoch: 365   loss: 5.497981368565745\n",
            "epoch: 366   loss: 7.356978780110956\n",
            "epoch: 367   loss: 3.864537591424545\n",
            "epoch: 368   loss: 4.950594089853743\n",
            "epoch: 369   loss: 7.8375550059564345\n",
            "epoch: 370   loss: 4.794136431076338\n",
            "epoch: 371   loss: 4.444345494255014\n",
            "epoch: 372   loss: 7.151689156923636\n",
            "epoch: 373   loss: 5.259443139496637\n",
            "epoch: 374   loss: 4.087464679915544\n",
            "epoch: 375   loss: 6.607168403170176\n",
            "epoch: 376   loss: 6.688445083826302\n",
            "epoch: 377   loss: 3.9828404728579265\n",
            "epoch: 378   loss: 5.611274316005569\n",
            "epoch: 379   loss: 7.088753428960223\n",
            "epoch: 380   loss: 4.082649933884247\n",
            "epoch: 381   loss: 5.406712168631421\n",
            "epoch: 382   loss: 7.051674922962594\n",
            "epoch: 383   loss: 5.1355430283089465\n",
            "epoch: 384   loss: 4.822964369967989\n",
            "epoch: 385   loss: 6.377180229168976\n",
            "epoch: 386   loss: 5.953354865146748\n",
            "epoch: 387   loss: 4.653510106686175\n",
            "epoch: 388   loss: 6.102638246102866\n",
            "epoch: 389   loss: 6.583090268971442\n",
            "epoch: 390   loss: 4.712915033024113\n",
            "epoch: 391   loss: 5.875810470012073\n",
            "epoch: 392   loss: 6.455995127327393\n",
            "epoch: 393   loss: 4.958559688471101\n",
            "epoch: 394   loss: 5.597188429205642\n",
            "epoch: 395   loss: 6.207668073067114\n",
            "epoch: 396   loss: 5.987578539706614\n",
            "epoch: 397   loss: 5.22538547635696\n",
            "epoch: 398   loss: 6.234251801794812\n",
            "epoch: 399   loss: 5.9668646102810285\n",
            "epoch: 400   loss: 4.855954319776065\n",
            "epoch: 401   loss: 6.279095962998396\n",
            "epoch: 402   loss: 6.044470303887633\n",
            "epoch: 403   loss: 5.252902609398091\n",
            "epoch: 404   loss: 6.067362847711381\n",
            "epoch: 405   loss: 5.980531221093333\n",
            "epoch: 406   loss: 5.661368996069296\n",
            "epoch: 407   loss: 5.385035904754625\n",
            "epoch: 408   loss: 5.940363658660058\n",
            "epoch: 409   loss: 5.943285252619005\n",
            "epoch: 410   loss: 5.1849668146084\n",
            "epoch: 411   loss: 6.436923700881927\n",
            "epoch: 412   loss: 6.04007861598613\n",
            "epoch: 413   loss: 5.159222224251142\n",
            "epoch: 414   loss: 6.017843653385632\n",
            "epoch: 415   loss: 5.903891914728096\n",
            "epoch: 416   loss: 5.499550928559951\n",
            "epoch: 417   loss: 5.885223301359645\n",
            "epoch: 418   loss: 6.056485651334444\n",
            "epoch: 419   loss: 5.953301828741419\n",
            "epoch: 420   loss: 5.224337067824359\n",
            "epoch: 421   loss: 6.086580541600609\n",
            "epoch: 422   loss: 5.904109797594696\n",
            "epoch: 423   loss: 5.521232474287665\n",
            "epoch: 424   loss: 6.119127681136475\n",
            "epoch: 425   loss: 6.071654639758634\n",
            "epoch: 426   loss: 5.516789908013885\n",
            "epoch: 427   loss: 5.752625052292604\n",
            "epoch: 428   loss: 5.803373385611032\n",
            "epoch: 429   loss: 6.014199856191623\n",
            "epoch: 430   loss: 5.485003210656476\n",
            "epoch: 431   loss: 6.163555000450912\n",
            "epoch: 432   loss: 6.077113147064577\n",
            "epoch: 433   loss: 5.2264761526316565\n",
            "epoch: 434   loss: 6.003967425001573\n",
            "epoch: 435   loss: 5.7879410649256915\n",
            "epoch: 436   loss: 5.632568370189099\n",
            "epoch: 437   loss: 5.965237266695172\n",
            "epoch: 438   loss: 5.818011871148146\n",
            "epoch: 439   loss: 5.93598776661794\n",
            "epoch: 440   loss: 5.26690479198062\n",
            "epoch: 441   loss: 5.891388771532992\n",
            "epoch: 442   loss: 5.821129335565518\n",
            "epoch: 443   loss: 5.441627932282679\n",
            "epoch: 444   loss: 6.013167223704783\n",
            "epoch: 445   loss: 5.883541287341866\n",
            "epoch: 446   loss: 5.508244826181283\n",
            "epoch: 447   loss: 5.567670831577639\n",
            "epoch: 448   loss: 5.713831524436699\n",
            "epoch: 449   loss: 5.521449909012176\n",
            "epoch: 450   loss: 5.716145022022805\n",
            "epoch: 451   loss: 6.067152935790463\n",
            "epoch: 452   loss: 5.631889981788751\n",
            "epoch: 453   loss: 5.379585803591783\n",
            "epoch: 454   loss: 5.762530334030009\n",
            "epoch: 455   loss: 5.712457333973725\n",
            "epoch: 456   loss: 5.391413262922525\n",
            "epoch: 457   loss: 5.940848637993574\n",
            "epoch: 458   loss: 5.843622630128336\n",
            "epoch: 459   loss: 5.518581602958921\n",
            "epoch: 460   loss: 5.540612683959835\n",
            "epoch: 461   loss: 5.734468318307239\n",
            "epoch: 462   loss: 5.463719455249839\n",
            "epoch: 463   loss: 5.499076191310877\n",
            "epoch: 464   loss: 6.163036679298418\n",
            "epoch: 465   loss: 5.508381416561532\n",
            "epoch: 466   loss: 5.415287428979761\n",
            "epoch: 467   loss: 5.778497422817019\n",
            "epoch: 468   loss: 5.68293588881051\n",
            "epoch: 469   loss: 5.392943716234969\n",
            "epoch: 470   loss: 5.626110790236022\n",
            "epoch: 471   loss: 6.044700223499457\n",
            "epoch: 472   loss: 5.506480996116214\n",
            "epoch: 473   loss: 5.408742061356244\n",
            "epoch: 474   loss: 5.87762536966443\n",
            "epoch: 475   loss: 5.481959756478273\n",
            "epoch: 476   loss: 5.4534604185784055\n",
            "epoch: 477   loss: 5.898328453666479\n",
            "epoch: 478   loss: 5.7749025491313715\n",
            "epoch: 479   loss: 5.49615382656625\n",
            "epoch: 480   loss: 5.519491171164289\n",
            "epoch: 481   loss: 5.759342935425414\n",
            "epoch: 482   loss: 5.482994444935115\n",
            "epoch: 483   loss: 5.500202482499032\n",
            "epoch: 484   loss: 5.900063591465465\n",
            "epoch: 485   loss: 5.7410492551695205\n",
            "epoch: 486   loss: 5.309183747547084\n",
            "epoch: 487   loss: 5.676145614587313\n",
            "epoch: 488   loss: 5.531059997366334\n",
            "epoch: 489   loss: 5.4600448520324765\n",
            "epoch: 490   loss: 5.648047752361938\n",
            "epoch: 491   loss: 5.8606748958579304\n",
            "epoch: 492   loss: 5.600331277345039\n",
            "epoch: 493   loss: 5.399119475536197\n",
            "epoch: 494   loss: 5.681332350611199\n",
            "epoch: 495   loss: 5.562134112660534\n",
            "epoch: 496   loss: 5.311749818058628\n",
            "epoch: 497   loss: 5.847277298880101\n",
            "epoch: 498   loss: 5.780837466321672\n",
            "epoch: 499   loss: 5.594271698029783\n",
            "epoch: 500   loss: 5.429033079199535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW--4Yhz3FLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "f87bc32d-1a9d-4976-e0d6-5d9721708444"
      },
      "source": [
        "plt.plot(loss_epoch1, label = \"Squared L2; 100 Nodes\")\n",
        "plt.plot(loss_epoch2, label = \"Squared L2; 200 Nodes\")\n",
        "plt.plot(loss_epoch3, label = \"Squared L2; 400 Nodes\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llAtuXGj5Gox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "length = len(tst_data)\n",
        "outputs = []\n",
        "\n",
        "for num_data in range (length):\n",
        "    input = Variable(tst_data[num_data]).cuda()\n",
        "    # === forward propagation ===\n",
        "    output = model(input)\n",
        "    outputs.append(output)\n",
        "    loss = criterion(output, tst_data[num_data])\n",
        "    # === calculating loss ===\n",
        "    test_loss += np.sqrt(loss.item())\n",
        "    s += 1.\n",
        "\n",
        "loss_calculated = test_loss/s\n",
        "print('n: 100 ; loss: ' + str(loss_calculated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBhGqmUj5HNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "length = len(tst_data)\n",
        "outputs2 = []\n",
        "\n",
        "for num_data in range (length):\n",
        "    input = Variable(tst_data[num_data]).cuda()\n",
        "    # === forward propagation ===\n",
        "    output = model2(input)\n",
        "    outputs2.append(output)\n",
        "    loss = criterion(output, tst_data[num_data])\n",
        "    # === calculating loss ===\n",
        "    test_loss += np.sqrt(loss.item())\n",
        "    s += 1.\n",
        "\n",
        "loss_calculated = test_loss/s\n",
        "print('n: 200 ; loss: ' + str(loss_calculated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvesNRax5QcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "length = len(tst_data)\n",
        "outputs3 = []\n",
        "\n",
        "for num_data in range (length):\n",
        "    input = Variable(tst_data[num_data]).cuda()\n",
        "    # === forward propagation ===\n",
        "    output = model3(input)\n",
        "    outputs3.append(output)\n",
        "    loss = criterion(output, tst_data[num_data])\n",
        "    # === calculating loss ===\n",
        "    test_loss += np.sqrt(loss.item())\n",
        "    s += 1.\n",
        "\n",
        "loss_calculated = test_loss/s\n",
        "print('n: 400 ; loss: ' + str(loss_calculated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkyoPUKX5Y25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 9 - Preprocessing for visualizing reconstructions from each model\n",
        "\n",
        "# 9.0 - From input\n",
        "outputs0_array = []\n",
        "outputs0_array.append(test_set0[3])     # add 1\n",
        "outputs0_array.append(test_set0[7])     # add 2\n",
        "outputs0_array.append(test_set0[0])     # add 3\n",
        "outputs0_array.append(test_set0[2])     # add 4\n",
        "outputs0_array.append(test_set0[1])     # add 5\n",
        "outputs0_array.append(test_set0[14])    # add 6\n",
        "outputs0_array.append(test_set0[8])     # add 7\n",
        "outputs0_array.append(test_set0[6])     # add 8\n",
        "outputs0_array.append(test_set0[5])     # add 9\n",
        "outputs0_array.append(test_set0[18])    # add 0\n",
        "\n",
        "# 9.1 - From model 1\n",
        "outputs_array = []\n",
        "outputs_array.append(outputs[3].cpu().detach().numpy())     # add 1\n",
        "outputs_array.append(outputs[7].cpu().detach().numpy())     # add 2\n",
        "outputs_array.append(outputs[0].cpu().detach().numpy())     # add 3\n",
        "outputs_array.append(outputs[2].cpu().detach().numpy())     # add 4\n",
        "outputs_array.append(outputs[1].cpu().detach().numpy())     # add 5\n",
        "outputs_array.append(outputs[14].cpu().detach().numpy())    # add 6\n",
        "outputs_array.append(outputs[8].cpu().detach().numpy())     # add 7\n",
        "outputs_array.append(outputs[6].cpu().detach().numpy())     # add 8\n",
        "outputs_array.append(outputs[5].cpu().detach().numpy())     # add 9\n",
        "outputs_array.append(outputs[18].cpu().detach().numpy())    # add 0\n",
        "\n",
        "# 9.2 - From model 2\n",
        "outputs2_array = []\n",
        "outputs2_array.append(outputs2[3].cpu().detach().numpy())     # add 1\n",
        "outputs2_array.append(outputs2[7].cpu().detach().numpy())     # add 2\n",
        "outputs2_array.append(outputs2[0].cpu().detach().numpy())     # add 3\n",
        "outputs2_array.append(outputs2[2].cpu().detach().numpy())     # add 4\n",
        "outputs2_array.append(outputs2[1].cpu().detach().numpy())     # add 5\n",
        "outputs2_array.append(outputs2[14].cpu().detach().numpy())    # add 6\n",
        "outputs2_array.append(outputs2[8].cpu().detach().numpy())     # add 7\n",
        "outputs2_array.append(outputs2[6].cpu().detach().numpy())     # add 8\n",
        "outputs2_array.append(outputs2[5].cpu().detach().numpy())     # add 9\n",
        "outputs2_array.append(outputs2[18].cpu().detach().numpy())    # add 0\n",
        "\n",
        "# 9.3 - From model 3\n",
        "outputs3_array = []\n",
        "outputs3_array.append(outputs3[3].cpu().detach().numpy())     # add 1\n",
        "outputs3_array.append(outputs3[7].cpu().detach().numpy())     # add 2\n",
        "outputs3_array.append(outputs3[0].cpu().detach().numpy())     # add 3\n",
        "outputs3_array.append(outputs3[2].cpu().detach().numpy())     # add 4\n",
        "outputs3_array.append(outputs3[1].cpu().detach().numpy())     # add 5\n",
        "outputs3_array.append(outputs3[14].cpu().detach().numpy())    # add 6\n",
        "outputs3_array.append(outputs3[8].cpu().detach().numpy())     # add 7\n",
        "outputs3_array.append(outputs3[6].cpu().detach().numpy())     # add 8\n",
        "outputs3_array.append(outputs3[5].cpu().detach().numpy())     # add 9\n",
        "outputs3_array.append(outputs3[18].cpu().detach().numpy())    # add 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIC3N8ja5aUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_recon = []\n",
        "for ind in range(10):\n",
        "    img_rec = np.concatenate((outputs0_array[ind].reshape([28, 28]), \n",
        "                              outputs_array[ind].reshape([28, 28]),\n",
        "                              outputs2_array[ind].reshape([28, 28]),\n",
        "                              outputs3_array[ind].reshape([28, 28])), axis = 1)\n",
        "    plt.imshow(img_rec, cmap = \"gray\")\n",
        "    img_recon.append(img_rec)\n",
        "\n",
        "img_complete = img_recon[0]\n",
        "for app in range(9):\n",
        "    img_complete = np.concatenate((img_complete, img_recon[app+1]), axis = 0)\n",
        "plt.imshow(img_complete)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb2LxXr85fd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binaryVis(val): # val = cutoff value\n",
        "    img_complete_bin = img_complete\n",
        "    img_complete_bin[img_complete_bin > val] = 1\n",
        "    img_complete_bin[img_complete_bin < val] = 0\n",
        "    plt.imshow(img_complete_bin)\n",
        "\n",
        "binaryVis(0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiVvvvHg5iNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 11 - Transform model weights into matrix arrays\n",
        "\n",
        "# 11.1 - Converting weights from first model into arrays\n",
        "weights11 = model.full_connection0.weight.data.cpu().detach().numpy()\n",
        "weights12 = model.full_connection1.weight.data.cpu().detach().numpy()\n",
        "\n",
        "# 11.2 - Converting weights from first model into arrays\n",
        "weights21 = model2.full_connection0.weight.data.cpu().detach().numpy()\n",
        "weights22 = model2.full_connection1.weight.data.cpu().detach().numpy()\n",
        "\n",
        "# 11.3 - Converting weights from first model into arrays\n",
        "weights31 = model3.full_connection0.weight.data.cpu().detach().numpy()\n",
        "weights32 = model3.full_connection1.weight.data.cpu().detach().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY7Fv87W5km0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displayWeights(weights, index):\n",
        "    weights = weights[:, index].reshape([28, 28])\n",
        "    plt.imshow(weights, cmap='gray')\n",
        "    plt.show\n",
        "displayWeights(weights32, 13)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hXYYAAV5lRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displayWeights_full(weights):\n",
        "    imgs_list = []\n",
        "    for x_dim in range(20):\n",
        "        for y_dim in range(20):\n",
        "            if y_dim == 0 and x_dim == 0:\n",
        "                imgs = weights[:, 0].reshape([28, 28])\n",
        "            if y_dim == 0 and x_dim > 0:\n",
        "                imgs_list.append(imgs)\n",
        "                imgs = weights[:, x_dim].reshape([28, 28])\n",
        "            if y_dim > 0:\n",
        "                imgs = np.concatenate((imgs, weights[:, x_dim + y_dim].reshape([28, 28])), axis = 1)\n",
        "    \n",
        "    imgs_complete = imgs_list[0]\n",
        "    for x_dim2 in range(19):\n",
        "        print(len(imgs_complete))\n",
        "        print(len(imgs_list[x_dim2]))\n",
        "        imgs_complete = np.concatenate((imgs_complete, imgs_list[x_dim2]), axis = 0)\n",
        "    \n",
        "    plt.imshow(imgs_complete, cmap = \"gray\")\n",
        "\n",
        "displayWeights_full(weights12)  # model with 100 nodes\n",
        "displayWeights_full(weights22)  # model with 200 nodes\n",
        "displayWeights_full(weights32)  # model with 400 nodes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS96P4qV5nnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 13 - Calculate the sparseness of hidden layer representations\n",
        "\n",
        "def SparsenessCalc(weights, val):\n",
        "    weights_s = weights.ravel()\n",
        "    weights_s[weights_s > val] = 0\n",
        "    weights_s[weights_s < -val] = 0\n",
        "    weights_s[weights_s > 0] = 1\n",
        "    weights_s[weights_s < 0] = 1\n",
        "    return sum(weights_s)/len(weights_s)\n",
        "\n",
        "print(\"cutoff value: 1e-5\")\n",
        "print(\"model 1: \" + str(SparsenessCalc(weights12.copy(), 1e-5)))\n",
        "print(\"model 2: \" + str(SparsenessCalc(weights22.copy(), 1e-5)))\n",
        "print(\"model 3: \" + str(SparsenessCalc(weights32.copy(), 1e-5)))\n",
        "sum_sparse = SparsenessCalc(weights12.copy(), 1e-5) + SparsenessCalc(weights22.copy(), 1e-5) + SparsenessCalc(weights32.copy(), 1e-5)\n",
        "print(\"average: \" + str(sum_sparse/3))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}